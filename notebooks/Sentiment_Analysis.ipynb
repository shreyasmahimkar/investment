{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/smahimkar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/smahimkar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/smahimkar/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What', 'can', 'I', 'say', 'about', 'this', 'place', '.'], ['The', 'staff', 'of', 'the', 'restaurant', 'is', 'nice', 'and', 'the', 'eggplant', 'is', 'not', 'bad', '.'], ['Apart', 'from', 'that', ',', 'very', 'uninspired', 'food', ',', 'lack', 'of', 'atmosphere', 'and', 'too', 'expensive', '.'], ['I', 'am', 'a', 'staunch', 'vegetarian', 'and', 'was', 'sorely', 'dissapointed', 'with', 'the', 'veggie', 'options', 'on', 'the', 'menu', '.'], ['Will', 'be', 'the', 'last', 'time', 'I', 'visit', ',', 'I', 'recommend', 'others', 'to', 'avoid', '.']]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. \\\n",
    "Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was \\\n",
    "sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "splitted_sentences = splitter.split(text)\n",
    "\n",
    "print(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('What', 'What', ['WP']), ('can', 'can', ['MD']), ('I', 'I', ['PRP']), ('say', 'say', ['VBP']), ('about', 'about', ['IN']), ('this', 'this', ['DT']), ('place', 'place', ['NN']), ('.', '.', ['.'])], [('The', 'The', ['DT']), ('staff', 'staff', ['NN']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('restaurant', 'restaurant', ['NN']), ('is', 'is', ['VBZ']), ('nice', 'nice', ['JJ']), ('and', 'and', ['CC']), ('the', 'the', ['DT']), ('eggplant', 'eggplant', ['NN']), ('is', 'is', ['VBZ']), ('not', 'not', ['RB']), ('bad', 'bad', ['JJ']), ('.', '.', ['.'])], [('Apart', 'Apart', ['RB']), ('from', 'from', ['IN']), ('that', 'that', ['IN']), (',', ',', [',']), ('very', 'very', ['RB']), ('uninspired', 'uninspired', ['JJ']), ('food', 'food', ['NN']), (',', ',', [',']), ('lack', 'lack', ['NN']), ('of', 'of', ['IN']), ('atmosphere', 'atmosphere', ['NN']), ('and', 'and', ['CC']), ('too', 'too', ['RB']), ('expensive', 'expensive', ['JJ']), ('.', '.', ['.'])], [('I', 'I', ['PRP']), ('am', 'am', ['VBP']), ('a', 'a', ['DT']), ('staunch', 'staunch', ['NN']), ('vegetarian', 'vegetarian', ['NN']), ('and', 'and', ['CC']), ('was', 'was', ['VBD']), ('sorely', 'sorely', ['RB']), ('dissapointed', 'dissapointed', ['VBN']), ('with', 'with', ['IN']), ('the', 'the', ['DT']), ('veggie', 'veggie', ['NN']), ('options', 'options', ['NNS']), ('on', 'on', ['IN']), ('the', 'the', ['DT']), ('menu', 'menu', ['NN']), ('.', '.', ['.'])], [('Will', 'Will', ['MD']), ('be', 'be', ['VB']), ('the', 'the', ['DT']), ('last', 'last', ['JJ']), ('time', 'time', ['NN']), ('I', 'I', ['PRP']), ('visit', 'visit', ['VBP']), (',', ',', [',']), ('I', 'I', ['PRP']), ('recommend', 'recommend', ['VBP']), ('others', 'others', ['NNS']), ('to', 'to', ['TO']), ('avoid', 'avoid', ['VB']), ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "\n",
    "print(pos_tagged_sentences)\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
